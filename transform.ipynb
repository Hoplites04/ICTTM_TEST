{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8f0f9-e853-44ff-a034-193a76fae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 pyspark delta-spark python-dotenv\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffafa3-a966-4eff-beab-75fca04d5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, types\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f66ea-a1de-4ade-9eb4-6314ff85a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_storage_access_key = os.getenv('OBJ_STORAGE_ACCESS_KEY', 'uZCPe6xoPsMQymYaBwPk')\n",
    "obj_storage_secret_key = os.getenv('OBJ_STORAGE_SECRET_KEY', 'iLA2wO10sexC0RzUHodV0xUpTqXPFJnfubXFGJBt')\n",
    "obj_storage_endpoint = os.getenv('OBJ_STORAGE_ENDPOINT', 'http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d3cb3-8651-4422-8c60-5fe277cc9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = \"s3a://data/data-raw/data.json\"\n",
    "path_2 = \"s3a://data/data-raw/data2.json\"\n",
    "path_3 = \"s3a://data/data-raw/data3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64428df0-bdd4-48df-8f29-f298a6285791",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", 'uZCPe6xoPsMQymYaBwPk') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", 'iLA2wO10sexC0RzUHodV0xUpTqXPFJnfubXFGJBt') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", 'http://localhost:9000') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b79d24-7a96-44b4-a560-49871e5f9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading each file in the bucket\n",
    "\n",
    "df1 = spark.read.option(\"multiline\", \"true\") \\\n",
    "    .json(path_1)\n",
    "df2 = spark.read.option(\"multiline\", \"true\") \\\n",
    "    .json(path_2)\n",
    "df3 = spark.read.option(\"multiline\", \"true\") \\\n",
    "    .json(path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b70bcd-2ec9-4605-9294-7f6594f35502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing each dataframe after reading\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf7f94-b008-470f-aca1-6920aaf0b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function from the \"download.ipynb\" to upload the result.json\n",
    "\n",
    "def upload_file_to_minio(file_path, minio_bucket, minio_object_name):\n",
    "    s3c = boto3.resource('s3',\n",
    "                        endpoint_url=obj_storage_endpoint,\n",
    "                        aws_access_key_id=obj_storage_access_key,\n",
    "                        aws_secret_access_key=obj_storage_secret_key,\n",
    "                        config=boto3.session.Config(signature_version='s3v4'),\n",
    "                        verify=False\n",
    "                        )\n",
    "    s3c.Bucket(minio_bucket).upload_file(file_path, minio_object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8dabf-f699-4fa2-a8ef-94399a62364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging each dataframe from data.json, data2.json, data3.json\n",
    "\n",
    "merged_df = df1.union(df2)\n",
    "merged_df1 = merged_df.union(df3)\n",
    "merged_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223c3af-b706-4d83-a2b0-0e8182cce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the PySpark dataframe to Pandas dataframe, the reason was I continously ate error in the face while trying write JSON file by Spark =)\n",
    "\n",
    "print(type(merged_df1));\n",
    "result = merged_df1.toPandas()\n",
    "print(type(result));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35493d-5018-47e3-b15d-7e11dfaba55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the result.json DataFrame and uploading it to MinIO\n",
    "\n",
    "result.to_json('result.json', orient='records')\n",
    "display(result)\n",
    "upload_file_to_minio('result.json', 'data', 'data-result/result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be84fb-e26f-440b-af9e-1a6ccfe0dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the result.json from MinIO then remove the duplicated case\n",
    "\n",
    "path_4 = \"s3a://data/data-result/result.json\"\n",
    "\n",
    "df4 = spark.read \\\n",
    "    .json(path_4)\n",
    "df4.show()\n",
    "\n",
    "final_result = df4.drop_duplicates(subset=['id'])\n",
    "final_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
